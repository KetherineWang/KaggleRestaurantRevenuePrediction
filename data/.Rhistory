)
sales_by_store
p_store_mean <- ggplot(sales_by_store,
aes(x = factor(Store), y = mean_sales)) +
geom_col() +
scale_y_continuous(labels = dollar_format()) +
labs(
title = "Average Weekly Sales by Store",
x = "Store",
y = "Average Weekly Sales"
) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
p_store_mean
holiday_summary <- train %>%
group_by(IsHoliday) %>%
summarise(
n_weeks     = n(),
mean_sales  = mean(Weekly_Sales),
median_sales= median(Weekly_Sales),
sd_sales    = sd(Weekly_Sales),
.groups = "drop"
)
holiday_summary
p_holiday_hist
p_holiday_hist <- ggplot(train,
aes(x = Weekly_Sales,
fill = IsHoliday)) +
geom_histogram(position = "identity",
alpha = 0.5,
bins = 40) +
scale_x_continuous(
labels = dollar_format(),
## Clip extreme values so we can see the bulk distribution
limits = c(0, quantile(train$Weekly_Sales, 0.99, na.rm = TRUE))
) +
labs(
title = "Weekly Sales Distribution: Holiday vs Non-Holiday Weeks",
x = "Weekly Sales",
y = "Count of Weeks",
fill = "Is Holiday?"
)
p_holiday_hist
library(tidymodels)
tidymodels_prefer() # Tells R to prefer tidymodels functions over other packages
set.seed(348)
train_file <- "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/data/train.csv"
test_file <- "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/data/test.csv"
train <- readr::read_csv(train_file)
test <- readr::read_csv(test_file)
train <- train %>%
mutate(
type = factor(type),
color = factor(color)
)
set.seed(348)
cv_folds <- vfold_cv(train, v = 5, strata = type)
ggg_recipe <- recipe(type ~ ., data = train) %>%
update_role(id, new_role = "id") %>%   # Keep id as non-predictor
step_rm(id) %>%   # Remove id from predictors
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
###
ghoul_prep <- prep(ghoul_rec)
###
ghoul_prep <- prep(ghoul_recipe)
###
ggg_prep <- prep(ggg_recipe)
ggg_train_processed <- juice(ggg_prep)
ggg_boost_spec <- boost_tree(
mode           = "classification",
trees          = tune(),
tree_depth     = tune(),
learn_rate     = tune(),
loss_reduction = tune(),  # gamma
min_n          = tune(),
mtry           = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
param_set <- parameters(ggg_boost_spec) %>%
update(
mtry = finalize(
mtry(),
ggg_train_processed %>% select(-type)
)
)
param_set <- parameters(ggg_wf) %>%
update(
mtry = finalize(
mtry(),
ggg_train_processed %>% select(-type)
)
)
param_set <- parameters(ggg_wf) %>%
update(
mtry = finalize(
mtry(),
ggg_train_processed %>% dplyr::select(-type)
)
)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
trees(),
tree_depth(),
learn_rate(),
loss_reduction(),
min_n(),
mtry(range = c(1L, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_latin_hypercube(
param_set,
size = 20
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 20
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
install.packages("xgboost")
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, "accuracy")
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ghoul_wf, best_params)
final_fit <- fit(final_wf, data = train)
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
library(tidymodels)
tidymodels_prefer()   # Tells R to prefer tidymodels functions over other packages
set.seed(348)
train_file <- "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/data/train.csv"
test_file <- "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/data/test.csv"
train <- readr::read_csv(train_file)
test <- readr::read_csv(test_file)
train <- train %>%
mutate(
type = factor(type),
color = factor(color)
)
set.seed(348)
cv_folds <- vfold_cv(train, v = 5, strata = type)   # Stratified sampling ensures each fold has roughly the same class proportions as the full dataset
ggg_recipe <- recipe(type ~ ., data = train) %>%
update_role(id, new_role = "id") %>%   # Keep id as non-predictor
step_rm(id) %>%   # Remove id from predictors
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())   # Remove any predictor with zero variance which is constant
ggg_prep <- prep(ggg_recipe)
ggg_train_processed <- juice(ggg_prep)
###
ggg_boost_spec <- boost_tree(
mode           = "classification",
trees          = tune(),
tree_depth     = tune(),
learn_rate     = tune(),
loss_reduction = tune(),  # gamma
min_n          = tune(),
mtry           = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
trees(),
tree_depth(range = c(2, 10)),
learn_rate(range = c(-5, -1)),
loss_reduction(),
min_n(),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 60
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
trees(),
tree_depth(),
learn_rate(range = c(-5, -1)),
loss_reduction(),
min_n(),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 50
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
tree_depth(range = c(2, 6)),
loss_reduction(range = c(0, 5)),
min_n(range = c(5, 30)),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 30
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
ggg_boost_spec <- boost_tree(
mode = "classification",
trees = 300,
tree_depth = tune(),
learn_rate = 0.5,
loss_reduction = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
tree_depth(range = c(2, 6)),
loss_reduction(range = c(0, 5)),
min_n(range = c(5, 30)),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 30
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
ggg_boost_spec <- boost_tree(
mode = "classification",
trees = 300,
tree_depth = tune(),
learn_rate = 0.3,
loss_reduction = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
tree_depth(range = c(1, 6)),
loss_reduction(range = c(0, 5)),
min_n(range = c(10, 30)),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 30
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
ggg_boost_spec <- boost_tree(
mode = "classification",
trees = 300,
tree_depth = tune(),
learn_rate = 0.1,
loss_reduction = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
tree_depth(range = c(1, 6)),
loss_reduction(range = c(0, 5)),
min_n(range = c(10, 30)),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 30
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
ggg_boost_spec <- boost_tree(
mode = "classification",
trees = 400,
tree_depth = tune(),
learn_rate = 0.3,
loss_reduction = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
tree_depth(range = c(1, 6)),
loss_reduction(range = c(0, 5)),
min_n(range = c(20, 50)),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 40
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
ggg_boost_spec <- boost_tree(
mode = "classification",
trees = tune(),
tree_depth = tune(),
learn_rate = 0.3,
loss_reduction = tune(),
min_n = tune(),
mtry = tune()
) %>%
set_engine("xgboost")
ggg_wf <- workflow() %>%
add_model(ggg_boost_spec) %>%
add_recipe(ggg_recipe)
library(dials)
# ggg_train_processed is the juiced data from your recipe
n_pred <- ncol(ggg_train_processed) - 1  # subtract outcome column
param_set <- dials::parameters(
trees(),
tree_depth(),
loss_reduction(range = c(0, 5)),
min_n(),
mtry(range = c(1, n_pred))
)
set.seed(348)
tune_grid_vals <- grid_space_filling(
param_set,
size = 20
)
set.seed(348)
tune_res <- tune_grid(
ggg_wf,
resamples = cv_folds,
grid      = tune_grid_vals,
metrics   = metric_set(accuracy)
)
best_params <- select_best(tune_res, metric = "accuracy")
collect_metrics(tune_res) %>%
arrange(desc(mean)) %>%
head()
final_wf <- finalize_workflow(ggg_wf, best_params)
final_fit <- fit(final_wf, data = train)
test_preds <- predict(final_fit, new_data = test)
submission <- test %>%
select(id) %>%
bind_cols(type = test_preds$.pred_class)
readr::write_csv(submission, "C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleGhoulsGoblinsGhosts/submissions.csv")
print("Best Parameters:\n")
print(f"Best Parameters:")
print("Best Parameters:")
print(best_params)
print(test_preds)
source("~/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/restaurant_model.R", echo = TRUE)
setwd("C:/Users/HongtingWang/Documents/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/data")
source("~/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/restaurant_model.R", echo = TRUE)
source("~/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/restaurant_model.R", echo = TRUE)
print(head(train))
source("~/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/restaurant_model.R", echo = TRUE)
source("~/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/restaurant_model.R", echo = TRUE)
source("~/STAT 348 - Predictive Analytics/KaggleRestaurantRevenuePrediction/restaurant_model.R", echo = TRUE)
